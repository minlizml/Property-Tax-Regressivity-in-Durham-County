select(
NAME, GEOG,
ASSESSMENT_CHANGE,
SALES_QUARTILE,
OVER_UNDER,
D_OVER_UNDER,
COUNT, D_COUNT,
MED_SALES_PRICE,
MED_SALES_RATIO,
COD, D_COD, PRD,
VCS_LOW_COUNT
)
# Durham County financial table with CORRECTED grouping
durham_financial <- final_df %>%
# First calculate all statistics at the county level
summarise(
ASSESSMENT_CHANGE = mean(ASSESSMENT_CHANGE, na.rm = TRUE),
MED_SALES_PRICE = median(PRICE, na.rm = TRUE),
MED_SALES_RATIO = median(SALES_RATIO, na.rm = TRUE),
MEAN_SALES_RATIO = mean(SALES_RATIO, na.rm = TRUE),
AVG_ABS_DIFF = mean(abs(SALES_RATIO - median(SALES_RATIO, na.rm = TRUE)), na.rm = TRUE),
COD = (AVG_ABS_DIFF / median(SALES_RATIO, na.rm = TRUE)) * 100,
D_COD = mean(abs(SALES_RATIO - median(final_df$SALES_RATIO, na.rm = TRUE)), na.rm = TRUE) /
median(final_df$SALES_RATIO, na.rm = TRUE) * 100,
SUM_SALE = sum(PRICE, na.rm = TRUE),
SUM_ASSD = sum(SALE_YEAR_ASSESSED_VALUE, na.rm = TRUE),
WEIGHTED_RATIO = SUM_ASSD / SUM_SALE,
PRD = MEAN_SALES_RATIO / WEIGHTED_RATIO,
TOTAL_COUNT = n()
) %>%
# Now join with the count data which is grouped by additional dimensions
# Using cross_join because we want county-level stats to appear for each quartile/over_under combination
cross_join(
final_df %>%
group_by(D_SALES_QUARTILE, D_OVER_UNDER) %>%
summarise(
COUNT = n(),
D_COUNT = n(),
.groups = "drop"
)
) %>%
mutate(
NAME = "Durham County",
GEOG = "county",
SALES_QUARTILE = D_SALES_QUARTILE,
OVER_UNDER = D_OVER_UNDER
) %>%
select(
NAME, GEOG,
ASSESSMENT_CHANGE,
SALES_QUARTILE,
OVER_UNDER,
D_OVER_UNDER = OVER_UNDER,
COUNT, D_COUNT,
MED_SALES_PRICE,
MED_SALES_RATIO,
COD, D_COD, PRD
)
# Combine all data
financial_data <- bind_rows(
neighborhood_financial,
vcs_financial,
durham_financial
)
# Save all files
write.csv(financial_data, "financial_data.csv", row.names = FALSE)
# Load packages
library(dplyr)
library(readxl)
library(readr)
library(lubridate)
library(tidyr)
library(stringr)
library(writexl)
# Load data
property_2024_df <- read_excel("2021-2024_assessments.xlsx", sheet = "2024")
property_2023_df <- read_excel("2021-2024_assessments.xlsx", sheet = "2023")
property_2022_df <- read_excel("2021-2024_assessments.xlsx", sheet = "2022")
property_2021_df <- read_excel("2021-2024_assessments.xlsx", sheet = "2021")
property_2025_df <- read_csv("2025 Assessments.csv", col_select = c("REID", "TOTAL_PROP_VALUE"), col_types = cols(REID = col_character()))
sales_df <- read_excel("qualified-residential-sales-info-2021-2024.xlsx")
blockgroups_df <- read_csv("2024_10_All_Durham_BlockGroups_with_race_neighborhoods.csv", col_select = c("REID", "NEIGHBORHOOD", "race_asian_per", "race_black_per", "race_hispanic_per", "race_white_per", "name_2"), col_types = cols(REID = col_character()))
# Make sure REID is always a text field
sales_df <- sales_df %>%
mutate(REID = as.character(REID))
# For the 2024 assessment table, keep only residential properties assessed at at least $20,000
# Filter out Townhouses and Condos
property_2024_df <- property_2024_df %>%
filter(PROPERTY_CLASS == "RESIDENTIAL"
& startsWith(LAND_CLASS, "RES/") & !startsWith(LAND_CLASS, "RES/ CONDO") & !startsWith(LAND_CLASS, "RES/TWNH")
& TOTAL_PROP_VALUE_ASSD >= 20000)
# Create main data form by adding all other assessment year values to the 2024 table
main_df <- property_2024_df %>%
select(
REID = PARCEL_NUM,
PIN = PIN_NUM,
ADDRESS = LOCATION_ADDR,
LAND_CLASS,
ASSESSED_VALUE_2024 = TOTAL_PROP_VALUE_ASSD,
VCS
) %>%
left_join(
property_2025_df %>%
select(REID, ASSESSED_VALUE_2025 = TOTAL_PROP_VALUE),
by = "REID"
)%>%
left_join(
property_2021_df %>%
select(REID = PARCEL_NUM, ASSESSED_VALUE_2021 = TOTAL_PROP_VALUE_ASSD),
by = "REID"
) %>%
left_join(
property_2022_df %>%
select(REID = PARCEL_NUM, ASSESSED_VALUE_2022 = TOTAL_PROP_VALUE_ASSD),
by = "REID"
)%>%
left_join(
property_2023_df %>%
select(REID = PARCEL_NUM, ASSESSED_VALUE_2023 = TOTAL_PROP_VALUE_ASSD),
by = "REID"
)
# Calculation of change in assessed value
main_df <- main_df %>%
mutate(
ASSESSMENT_CHANGE = ((ASSESSED_VALUE_2025 - ASSESSED_VALUE_2024) / ASSESSED_VALUE_2024) * 100
)
# Filter out properties where assessment change was greater than 30,000% AND 2024 assessed value was < $10,000
# These properties are such extreme outliers they don't make sense to keep in the dataset
main_df <- main_df %>%
filter(!(ASSESSMENT_CHANGE > 30000 & ASSESSED_VALUE_2024 <10000))
# Add neighborhood names, race data, and VCS neighborhood
main_df <- main_df %>% inner_join(
blockgroups_df %>%
select(REID,
VCS_NEIGHBORHOOD = NEIGHBORHOOD,
PERCENT_ASIAN = race_asian_per,
PERCENT_BLACK = race_black_per,
PERCENT_HISPANIC = race_hispanic_per,
PERCENT_WHITE = race_white_per,
NEIGHBORHOOD = name_2),
by = "REID"
)
# Combine neighborhoods with low numbers
main_df <- main_df %>%
mutate(NEIGHBORHOOD = case_when(
NEIGHBORHOOD == "Bon Air Avenue" ~ "Colonial Village",
NEIGHBORHOOD == "NCCCU" ~ "College View / NCCU",
NEIGHBORHOOD == "North Carolina Central University" ~ "College View / NCCU",
NEIGHBORHOOD == "College View" ~ "College View / NCCU",
TRUE ~ NEIGHBORHOOD))
# Create Grant Street neighborhood from VCS = R830G
main_df <- main_df %>%
mutate(NEIGHBORHOOD = ifelse(VCS == "R830G", "Grant Street", NEIGHBORHOOD))
# Add VCS neighborhood suffix to VCS
main_df <- main_df %>%
mutate(VCS = ifelse(is.na(VCS_NEIGHBORHOOD),
VCS,
paste(VCS, VCS_NEIGHBORHOOD, sep = " - ")))
# Add County variable
main_df <- main_df %>%
mutate(COUNTY = "Durham County")
# Filter down to the latest sales records
sales_latest <- sales_df %>%
mutate(SALE_DATE = as.Date(SALE_DATE)) %>%
group_by(REID) %>%
slice_max(order_by = SALE_DATE, n = 1) %>%
ungroup()
# Filter down to the highest price for multiple sales recorded on the same date
sales_latest <- sales_latest %>%
group_by(REID) %>%
slice_max(order_by = PRICE, n = 1) %>%
ungroup()
# Delete duplicate records
sales_latest <- sales_latest %>%
distinct()
# Create sale year
sales_latest <- sales_latest %>%
mutate(SALE_YEAR = year(SALE_DATE))
# Merge sales data with main dataset
main_df <- main_df %>%
left_join(
sales_latest %>%
select(REID, PRICE, SALE_DATE, SALE_YEAR),
by = "REID"
)
# Find mean assessment change for Durham
# Needs to be done here before filtering out properties that did not sell
# Join back to main dataset later
d_mean_assess_change <- main_df %>% filter(!is.na(ASSESSMENT_CHANGE)) %>%
summarise(
D_MEAN_ASSESSMENT_CHANGE = mean(ASSESSMENT_CHANGE, na.rm = TRUE)
)
# Find mean assessment change for Neighborhood
# Needs to be done here before filtering out properties that did not sell
# Join back to main dataset later
n_mean_assess_change <- main_df %>%
filter(!is.na(NEIGHBORHOOD)) %>%
group_by(NEIGHBORHOOD) %>%
summarise(
N_MEAN_ASSESSMENT_CHANGE = mean(ASSESSMENT_CHANGE, na.rm = TRUE)
)
# Find mean assessment change for VCS
# Needs to be done here before filtering out properties that did not sell
# Join back to main dataset later
vcs_mean_assess_change <- main_df %>%
filter(!is.na(VCS)) %>%
group_by(VCS) %>%
summarise(
VCS_MEAN_ASSESSMENT_CHANGE = mean(ASSESSMENT_CHANGE, na.rm = TRUE)
)
# Create and save race_data table
neighborhood_race <- main_df %>%
filter(!is.na(NEIGHBORHOOD)) %>%
group_by(NEIGHBORHOOD) %>%
summarise(
Asian = mean(PERCENT_ASIAN, na.rm = TRUE),
Black = mean(PERCENT_BLACK, na.rm = TRUE),
Hispanic = mean(PERCENT_HISPANIC, na.rm = TRUE),
White = mean(PERCENT_WHITE, na.rm = TRUE)
) %>%
pivot_longer(
cols = c(Asian, Black, Hispanic, White),
names_to = "Race",
values_to = "Race_percentage"
) %>%
mutate(Name = NEIGHBORHOOD) %>%
select(Name, Race, Race_percentage)
vcs_race <- main_df %>%
filter(!is.na(VCS)) %>%
group_by(VCS) %>%
summarise(
Asian = mean(PERCENT_ASIAN, na.rm = TRUE),
Black = mean(PERCENT_BLACK, na.rm = TRUE),
Hispanic = mean(PERCENT_HISPANIC, na.rm = TRUE),
White = mean(PERCENT_WHITE, na.rm = TRUE)
) %>%
pivot_longer(
cols = c(Asian, Black, Hispanic, White),
names_to = "Race",
values_to = "Race_percentage"
) %>%
mutate(Name = VCS) %>%
select(Name, Race, Race_percentage)
durham_race <- main_df %>%
summarise(
Asian = mean(PERCENT_ASIAN, na.rm = TRUE),
Black = mean(PERCENT_BLACK, na.rm = TRUE),
Hispanic = mean(PERCENT_HISPANIC, na.rm = TRUE),
White = mean(PERCENT_WHITE, na.rm = TRUE)
) %>%
pivot_longer(
cols = c(Asian, Black, Hispanic, White),
names_to = "Race",
values_to = "Race_percentage"
) %>%
mutate(Name = "Durham County") %>%
select(Name, Race, Race_percentage)
race_data <- bind_rows(
neighborhood_race,
vcs_race,
durham_race)
# Remove race columns
main_df <- main_df %>%
select(-c(PERCENT_ASIAN, PERCENT_BLACK, PERCENT_HISPANIC, PERCENT_WHITE))
# Crate and save related_name table
related_name_1 <- main_df %>%
filter(!is.na(NEIGHBORHOOD) & !is.na(VCS)) %>%
select(NAME = NEIGHBORHOOD, RELATED_NAME = VCS) %>%
distinct()
related_name_2 <- main_df %>%
filter(!is.na(NEIGHBORHOOD) & !is.na(VCS)) %>%
select(NAME = VCS, RELATED_NAME = NEIGHBORHOOD) %>%
distinct()
related_name <- bind_rows(related_name_1, related_name_2)
write.csv(related_name, "related_name.csv", row.names = FALSE)
# Find which assessment value to use
# Some properties have no assessment value for the year in which they sold
# These will be filtered out of the dataset
main_df <- main_df %>%
mutate(SALE_YEAR_ASSESSED_VALUE = case_when(
SALE_YEAR == 2024 ~ ASSESSED_VALUE_2024,
SALE_YEAR == 2023 ~ ASSESSED_VALUE_2023,
SALE_YEAR == 2022 ~ ASSESSED_VALUE_2022,
SALE_YEAR == 2021 ~ ASSESSED_VALUE_2021)
)
# Calculate Sales Ratio and filter out all properties that have no SR
main_df <- main_df %>%
mutate(SALES_RATIO = SALE_YEAR_ASSESSED_VALUE / PRICE) %>%
filter(!is.na(SALES_RATIO))
# Output CSV main data file for data validation
write.csv(main_df, "main_df.csv", row.names = FALSE, na = "")
# Output CSV race and related names files
write.csv(race_data, "race_data.csv", row.names = FALSE, na = "")
write.csv(related_name, "related_name.csv", row.names = FALSE, na = "")
# Calculating neighborhood-based statistics
neighborhood_stats <- main_df %>%
# Calculate property count for each neighborhood
group_by(NEIGHBORHOOD) %>%
mutate(NEIGHBORHOOD_PROPERTY_COUNT = n()) %>%
# Remove neighborhoods with fewer than 5 sales
filter(NEIGHBORHOOD_PROPERTY_COUNT >= 5) %>%
# Add LOW_COUNT flag
mutate(
N_LOW_COUNT = ifelse(NEIGHBORHOOD_PROPERTY_COUNT >= 5 & NEIGHBORHOOD_PROPERTY_COUNT <= 9, "Y", NA),
N_MED_SALES_RATIO = median(SALES_RATIO, na.rm = TRUE),
N_ABS_SALES_RATIO = SALES_RATIO - N_MED_SALES_RATIO,
N_SALES_QUARTILE = paste0("Q", ntile(PRICE, 4))
) %>%
ungroup()
# Calculating VCS-based statistics
vcs_stats <- main_df %>%
# Calculate property count for each VCS
group_by(VCS) %>%
mutate(VCS_PROPERTY_COUNT = n()) %>%
# Remove VCS groups with fewer than 5 sales
filter(VCS_PROPERTY_COUNT >= 5) %>%
# Add LOW_COUNT flag
mutate(
VCS_LOW_COUNT = ifelse(VCS_PROPERTY_COUNT >= 5 & VCS_PROPERTY_COUNT <= 9, "Y", NA),
VCS_MED_SALES_RATIO = median(SALES_RATIO, na.rm = TRUE),
VCS_ABS_SALES_RATIO = SALES_RATIO - VCS_MED_SALES_RATIO,
VCS_SALES_QUARTILE = paste0("Q", ntile(PRICE, 4))
) %>%
ungroup()
# Calculating Durham County level statistics
durham_stats <- main_df %>%
mutate(
D_MED_SALES_RATIO = median(SALES_RATIO, na.rm = TRUE),
D_ABS_SALES_RATIO = SALES_RATIO - D_MED_SALES_RATIO,
D_OVER_UNDER = case_when(
abs(D_ABS_SALES_RATIO) <= 0.05 ~ "Accurate",
D_ABS_SALES_RATIO < -0.05 ~ "Underassessed",
D_ABS_SALES_RATIO > 0.05 ~ "Overassessed"
),
D_SALES_QUARTILE = paste0("Q", ntile(PRICE, 4))
)
# Merge all statistics
final_df <- neighborhood_stats %>%
left_join(
select(vcs_stats,
REID,
starts_with("VCS_")),
by = "REID"
) %>%
left_join(
select(durham_stats,
REID,
starts_with("D_")),
by = "REID"
)
# Create neighborhood financial data table
neighborhood_financial <- final_df %>%
filter(!is.na(NEIGHBORHOOD)) %>%
# First calculate all statistics at the NEIGHBORHOOD level only
group_by(NEIGHBORHOOD) %>%
# Ensure each neighborhood has at least 5 properties
filter(n() >= 5) %>%
summarise(
ASSESSMENT_CHANGE = mean(ASSESSMENT_CHANGE, na.rm = TRUE),
MED_SALES_PRICE = median(PRICE, na.rm = TRUE),
MED_SALES_RATIO = median(SALES_RATIO, na.rm = TRUE),
MEAN_SALES_RATIO = mean(SALES_RATIO, na.rm = TRUE),
AVG_ABS_DIFF = mean(abs(SALES_RATIO - median(SALES_RATIO, na.rm = TRUE)), na.rm = TRUE),
COD = (AVG_ABS_DIFF / median(SALES_RATIO, na.rm = TRUE)) * 100,
D_COD = mean(abs(SALES_RATIO - median(final_df$SALES_RATIO, na.rm = TRUE)), na.rm = TRUE) /
median(final_df$SALES_RATIO, na.rm = TRUE) * 100,
SUM_SALE = sum(PRICE, na.rm = TRUE),
SUM_ASSD = sum(SALE_YEAR_ASSESSED_VALUE, na.rm = TRUE),
WEIGHTED_RATIO = SUM_ASSD / SUM_SALE,
PRD = MEAN_SALES_RATIO / WEIGHTED_RATIO,
N_LOW_COUNT = if(any(N_LOW_COUNT == "Y", na.rm = TRUE)) "Y" else NA,
TOTAL_COUNT = n(),
.groups = "drop"
) %>%
# Join with the count data
left_join(
final_df %>%
filter(!is.na(NEIGHBORHOOD)) %>%
group_by(NEIGHBORHOOD, N_SALES_QUARTILE, D_OVER_UNDER) %>%
summarise(
COUNT = n(),
D_COUNT = n(),
.groups = "drop"
),
by = "NEIGHBORHOOD"
) %>%
mutate(
NAME = NEIGHBORHOOD,
GEOG = "neighborhood",
SALES_QUARTILE = N_SALES_QUARTILE,
OVER_UNDER = D_OVER_UNDER,
NID = row_number()
) %>%
select(
NAME, GEOG, NID,
ASSESSMENT_CHANGE,
SALES_QUARTILE,
OVER_UNDER,
D_OVER_UNDER,
COUNT, D_COUNT,
MED_SALES_PRICE,
MED_SALES_RATIO,
COD, D_COD, PRD,
N_LOW_COUNT
)
# Create VCS financial table
vcs_financial <- final_df %>%
filter(!is.na(VCS)) %>%
# First calculate all statistics at the VCS level only
group_by(VCS) %>%
# Ensure each VCS has at least 5 properties
filter(n() >= 5) %>%
summarise(
ASSESSMENT_CHANGE = mean(ASSESSMENT_CHANGE, na.rm = TRUE),
MED_SALES_PRICE = median(PRICE, na.rm = TRUE),
MED_SALES_RATIO = median(SALES_RATIO, na.rm = TRUE),
MEAN_SALES_RATIO = mean(SALES_RATIO, na.rm = TRUE),
AVG_ABS_DIFF = mean(abs(SALES_RATIO - median(SALES_RATIO, na.rm = TRUE)), na.rm = TRUE),
COD = (AVG_ABS_DIFF / median(SALES_RATIO, na.rm = TRUE)) * 100,
D_COD = mean(abs(SALES_RATIO - median(final_df$SALES_RATIO, na.rm = TRUE)), na.rm = TRUE) /
median(final_df$SALES_RATIO, na.rm = TRUE) * 100,
SUM_SALE = sum(PRICE, na.rm = TRUE),
SUM_ASSD = sum(SALE_YEAR_ASSESSED_VALUE, na.rm = TRUE),
WEIGHTED_RATIO = SUM_ASSD / SUM_SALE,
PRD = MEAN_SALES_RATIO / WEIGHTED_RATIO,
VCS_LOW_COUNT = if(any(VCS_LOW_COUNT == "Y", na.rm = TRUE)) "Y" else NA,
TOTAL_COUNT = n(),
.groups = "drop"
) %>%
# Join with the count data
left_join(
final_df %>%
filter(!is.na(VCS)) %>%
group_by(VCS, VCS_SALES_QUARTILE, D_OVER_UNDER) %>%
summarise(
COUNT = n(),
D_COUNT = n(),
.groups = "drop"
),
by = "VCS"
) %>%
mutate(
NAME = VCS,
GEOG = "vcs",
SALES_QUARTILE = VCS_SALES_QUARTILE,
OVER_UNDER = D_OVER_UNDER
) %>%
select(
NAME, GEOG,
ASSESSMENT_CHANGE,
SALES_QUARTILE,
OVER_UNDER,
D_OVER_UNDER,
COUNT, D_COUNT,
MED_SALES_PRICE,
MED_SALES_RATIO,
COD, D_COD, PRD,
VCS_LOW_COUNT
)
# Durham County financial table
durham_financial <- final_df %>%
# First calculate all statistics at the county level
summarise(
ASSESSMENT_CHANGE = mean(ASSESSMENT_CHANGE, na.rm = TRUE),
MED_SALES_PRICE = median(PRICE, na.rm = TRUE),
MED_SALES_RATIO = median(SALES_RATIO, na.rm = TRUE),
MEAN_SALES_RATIO = mean(SALES_RATIO, na.rm = TRUE),
AVG_ABS_DIFF = mean(abs(SALES_RATIO - median(SALES_RATIO, na.rm = TRUE)), na.rm = TRUE),
COD = (AVG_ABS_DIFF / median(SALES_RATIO, na.rm = TRUE)) * 100,
D_COD = mean(abs(SALES_RATIO - median(final_df$SALES_RATIO, na.rm = TRUE)), na.rm = TRUE) /
median(final_df$SALES_RATIO, na.rm = TRUE) * 100,
SUM_SALE = sum(PRICE, na.rm = TRUE),
SUM_ASSD = sum(SALE_YEAR_ASSESSED_VALUE, na.rm = TRUE),
WEIGHTED_RATIO = SUM_ASSD / SUM_SALE,
PRD = MEAN_SALES_RATIO / WEIGHTED_RATIO,
TOTAL_COUNT = n()
) %>%
# Join with the count data which is grouped by additional dimensions
cross_join(
final_df %>%
group_by(D_SALES_QUARTILE, D_OVER_UNDER) %>%
summarise(
COUNT = n(),
D_COUNT = n(),
.groups = "drop"
)
) %>%
mutate(
NAME = "Durham County",
GEOG = "county",
SALES_QUARTILE = D_SALES_QUARTILE,
OVER_UNDER = D_OVER_UNDER
) %>%
select(
NAME, GEOG,
ASSESSMENT_CHANGE,
SALES_QUARTILE,
OVER_UNDER,
D_OVER_UNDER = OVER_UNDER,
COUNT, D_COUNT,
MED_SALES_PRICE,
MED_SALES_RATIO,
COD, D_COD, PRD
)
# Combine all data
financial_data <- bind_rows(
neighborhood_financial,
vcs_financial,
durham_financial
)
# Save all files
write.csv(financial_data, "financial_data.csv", row.names = FALSE)
